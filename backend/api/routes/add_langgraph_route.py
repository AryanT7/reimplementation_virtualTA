from assistant_stream import create_run, RunController
from assistant_stream.serialization import DataStreamResponse
from langchain_core.messages import (
   HumanMessage,
   AIMessageChunk,
   AIMessage,
   ToolMessage,
   SystemMessage,
   BaseMessage,
)
from fastapi import FastAPI, Header
from pydantic import BaseModel
from typing import List, Literal, Union, Optional, Any
from ..utils.deps import get_current_user
from ..models.user import UserInDB
from fastapi import Depends, HTTPException, status
from ..database.mongodb import MongoDB
from bson import ObjectId
from langfuse.callback import CallbackHandler
from langfuse.decorators import observe
from langfuse import Langfuse
langfuse = Langfuse()


class LanguageModelTextPart(BaseModel):
   type: Literal["text"]
   text: str
   providerMetadata: Optional[Any] = None




class LanguageModelImagePart(BaseModel):
   type: Literal["image"]
   image: str  # Will handle URL or base64 string
   mimeType: Optional[str] = None
   providerMetadata: Optional[Any] = None




class LanguageModelFilePart(BaseModel):
   type: Literal["file"]
   data: str  # URL or base64 string
   mimeType: str
   providerMetadata: Optional[Any] = None




class LanguageModelToolCallPart(BaseModel):
   type: Literal["tool-call"]
   toolCallId: str
   toolName: str
   args: Any
   providerMetadata: Optional[Any] = None




class LanguageModelToolResultContentPart(BaseModel):
   type: Literal["text", "image"]
   text: Optional[str] = None
   data: Optional[str] = None
   mimeType: Optional[str] = None




class LanguageModelToolResultPart(BaseModel):
   type: Literal["tool-result"]
   toolCallId: str
   toolName: str
   result: Any
   isError: Optional[bool] = None
   content: Optional[List[LanguageModelToolResultContentPart]] = None
   providerMetadata: Optional[Any] = None




class LanguageModelSystemMessage(BaseModel):
   role: Literal["system"]
   content: str




class LanguageModelUserMessage(BaseModel):
   role: Literal["user"]
   content: List[
       Union[LanguageModelTextPart, LanguageModelImagePart, LanguageModelFilePart]
   ]




class LanguageModelAssistantMessage(BaseModel):
   role: Literal["assistant"]
   content: List[Union[LanguageModelTextPart, LanguageModelToolCallPart]]




class LanguageModelToolMessage(BaseModel):
   role: Literal["tool"]
   content: List[LanguageModelToolResultPart]




LanguageModelV1Message = Union[
   LanguageModelSystemMessage,
   LanguageModelUserMessage,
   LanguageModelAssistantMessage,
   LanguageModelToolMessage,
]




def convert_to_langchain_messages(
   messages: List[LanguageModelV1Message],
) -> List[BaseMessage]:
   result = []


   for msg in messages:
       if msg.role == "system":
           result.append(SystemMessage(content=msg.content))


       elif msg.role == "user":
           content = []
           for p in msg.content:
               if isinstance(p, LanguageModelTextPart):
                   content.append({"type": "text", "text": p.text})
               elif isinstance(p, LanguageModelImagePart):
                   content.append({"type": "image_url", "image_url": p.image})
           result.append(HumanMessage(content=content))


       elif msg.role == "assistant":
           # Handle both text and tool calls
           text_parts = [
               p for p in msg.content if isinstance(p, LanguageModelTextPart)
           ]
           text_content = " ".join(p.text for p in text_parts)
           tool_calls = [
               {
                   "id": p.toolCallId,
                   "name": p.toolName,
                   "args": p.args,
               }
               for p in msg.content
               if isinstance(p, LanguageModelToolCallPart)
           ]
           print(tool_calls)
           result.append(AIMessage(content=text_content, tool_calls=tool_calls))


       elif msg.role == "tool":
           for tool_result in msg.content:
               result.append(
                   ToolMessage(
                       content=str(tool_result.result),
                       tool_call_id=tool_result.toolCallId,
                   )
               )


   return result




class FrontendToolCall(BaseModel):
   name: str
   description: Optional[str] = None
   parameters: dict[str, Any]




class ChatRequest(BaseModel):
   system: Optional[str] = ""
   tools: Optional[List[FrontendToolCall]] = []
   messages: List[LanguageModelV1Message]


def add_langgraph_route(app: FastAPI, graph, path: str, current_user: UserInDB = Depends(get_current_user)):
 
   SYSTEM_MESSAGE = """
       # Professor's Teaching Assistant AI Agent


   You are an expert teaching assistant AI designed to help professors analyze course evaluations and improve their teaching methods. Your primary goal is to provide actionable insights and research-backed recommendations that enhance teaching effectiveness and student learning outcomes.


   ## Your Capabilities


   1. **get_evaluations_context**: You can analyze course evaluation data to identify patterns, strengths, and opportunities for improvement.


   2. **get_teaching_material_context**: You have access to "Teaching at Its Best," a comprehensive textbook on effective teaching practices in higher education, which you can reference to provide targeted advice. Do not use information from anywhere else.


   3. **fetch**: Only use this tool if the user has provided with a URL in the prompt.


   ## How You Operate


   When a professor shares course evaluation data with you:


   1. If the prompt is about the course evaluations then analyze the evaluations carefully to identify key themes, strengths, and areas for improvement.
   2. For each area that needs improvement, retrieve relevant guidance from get_teaching_material_context tool to provide research-backed recommendations.
   3. When appropriate, supplement this information with fetch tool.
   4. Present your insights in a clear, organized manner with specific, actionable steps the professor can take to improve.


   5. Maintain a supportive, constructive tone that acknowledges teaching strengths while suggesting improvements.


   ## Interaction Guidelines


   - Always respond with empathy and understanding of the challenges professors face.
   - Frame feedback positively as opportunities for growth rather than criticisms.
   - Provide specific, concrete examples when suggesting teaching strategies.
   - Prioritize quality over quantity in your recommendationsâ€”focus on the most impactful changes first.


   ## Special Instructions
   - when get_evaluations_context is used, make sure you also use get_teaching_material_context tool.


   You exist to help professors become more effective educators. Your ultimate measure of success is improved teaching practices that lead to better student learning outcomes.
      
   """
 
   async def chat_completions(request: ChatRequest, x_chat_id: Optional[str] = Header(None, alias="X-Chat-ID"), current_user: dict = Depends(get_current_user)):
       inputs = convert_to_langchain_messages(request.messages)
       # Check and update request count
       db = MongoDB.get_db()
       user = db.users.find_one({"_id": ObjectId(current_user.id)})
      
       if user["requests_used"] >= user["requests_limit"]:
           raise HTTPException(
               status_code=status.HTTP_429_TOO_MANY_REQUESTS,
               detail="Request limit exceeded"
           )
          
       # Increment request count
       db.users.update_one(
           {"_id": ObjectId(current_user.id)},
           {"$inc": {"requests_used": 1}}
       )
      
       inputs = convert_to_langchain_messages(request.messages)
       system_msg = SystemMessage(content=SYSTEM_MESSAGE)
       all_messages = [system_msg] + inputs


       print("inputs")
       print(inputs)


       accumulated_content = ""
       tool_calls = {} # Initialize an empty string to accumulate message content
      
       # Only create trace if user has enabled logging
       trace = None
       if current_user.enable_logging:
           trace = langfuse.trace(
               user_id = current_user.email,
               session_id = x_chat_id
           )
           trace.update(
               input = inputs[-1].content[0]['text'],
           )


       async def run(controller: RunController):
           tool_calls = {}
           tool_calls_by_idx = {}
           nonlocal accumulated_content  # Use nonlocal to modify the outer variable


           async for msg, metadata in graph.astream(
               {"messages": all_messages},
               config ={
                   "configurable": {
                       "system": request.system,
                       "frontend_tools": request.tools,
                       "metadata": {
                           "langfuse_session_id": x_chat_id,
                           "current_user": current_user.email,
                           "current_id": current_user.id
                       },
                   }
               },
               stream_mode="messages"
           ):
               if isinstance(msg, ToolMessage):
                   tool_controller = tool_calls.get(msg.tool_call_id)
                   if tool_controller is None:
                       # The MCP tool may send a ToolMessage before its call is registered.
                       # Register a fallback tool call using "MCP" (or an appropriate tool name) as a default.
                       tool_controller = await controller.add_tool_call("MCP", msg.tool_call_id)
                       tool_calls[msg.tool_call_id] = tool_controller
                  
                   # Accumulate tool message content
                   tool_controller.set_result(msg.content)


               if isinstance(msg, AIMessageChunk) or isinstance(msg, AIMessage):
                   if msg.content:
                       # Accumulate AI message content
                       accumulated_content += msg.content
                       controller.append_text(msg.content)


                   for chunk in msg.tool_call_chunks:
                       if not chunk["index"] in tool_calls_by_idx:
                           tool_controller = await controller.add_tool_call(
                               chunk["name"], chunk["id"]
                           )
                           tool_calls_by_idx[chunk["index"]] = tool_controller
                           tool_calls[chunk["id"]] = tool_controller
                       else:
                           tool_controller = tool_calls_by_idx[chunk["index"]]


                       tool_controller.append_args_text(chunk["args"])
          
           # After processing all message chunks, update the trace with the complete accumulated content
           if trace is not None:
               trace.update(
                   output = accumulated_content
               )


       return DataStreamResponse(create_run(run))


   app.add_api_route(path, chat_completions, methods=["POST"])